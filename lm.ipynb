{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d2c524-1691-40f8-8a2c-082ff04911e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0   744k      0  0:00:01  0:00:01 --:--:--  744k\n"
     ]
    }
   ],
   "source": [
    "#we always start with a dataset to train on. Let's download the tiny shakespeare dataset #!wget -> is for linux\n",
    "!curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2a4936-bba3-4d5e-a46f-822b3e7de739",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "#Explanation:\n",
    "\n",
    "# open('input.txt', 'r', encoding='utf-8') → Opens the file named input.txt in read mode ('r') using UTF-8 encoding.\n",
    "# with statement → Ensures that the file is properly closed after it is used.\n",
    "# as f → Assigns the opened file object to the variable f, allowing you to read its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb15b1b9-b381-4e0d-91a9-858bbf33b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ac9fbe-096c-4c75-8306-f5c2d9727ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "#lets look at the first 1000 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66a2b7e-6135-478c-9c7c-81aae6c4fe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#here are all the unique characters that occurs in this text\n",
    "chars = sorted(list(set(text)))    \n",
    "#when I call the set Constructor on it I'm just going to get the set of all the characters that occur in this text\n",
    "#and then I call list on that to create a list of those characters instead of just a set \n",
    "#so that I have an ordering an arbitrary ordering and\n",
    "#then I sort that so basically we get just all the characters that occur in the entire data set\n",
    "\n",
    "vocab_size = len(chars)\n",
    "#these are the possible elements of our sequences\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45044bb-f71e-4188-8b92-00c8b0838804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "#next we will would like to develop some strategy to tokenize the input text\n",
    "#now when people say tokenize they mean convert the raw text as a string to some sequence of integers\n",
    "#here we are building a character level language model, so we simply going to translate individual char into int\n",
    "\n",
    "#create a mapping from characters to integers\n",
    "stoi = {ch:i for i,ch in enumerate(chars) }         \n",
    "itos = {i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]                     #takes a string s as input and returns a list of integers \n",
    "decode = lambda l: ''.join([itos[i] for i in l])            #lambda is an anonymous function\n",
    "\n",
    "print(encode(\"hii there\"))                                  #we are going to receive a list of integers\n",
    "print(decode(encode(\"hii there\")))\n",
    "\n",
    "# stoi (string to integer): Maps characters to numbers.\n",
    "# itos (integer to string): Maps numbers back to characters.\n",
    "# encode: Converts a string into a list of integers using stoi.\n",
    "# decode: Converts a list of integers back into a string using itos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e1846c-c958-4daa-814f-2ae58b602b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n",
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "#we're going to take all of the text in tiny Shakespeare, encode it \n",
    "#and then wrap it into a torch. tensor to get the data tensor\n",
    "\n",
    "#let's now encode the entire text dataset and store in into a torch.Tensor\n",
    "#!pip install torch torchvision torchaudio \n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "print(torch.__version__)\n",
    "data = torch.tensor(encode(text), dtype= torch.long)\n",
    "#Long tensors (torch.long) are typically used for categorical data, like word indices in NLP tasks.\n",
    "\n",
    "print(data.shape,data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329d3583-f53c-4afd-b351-1716551cd1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c5e31c-b817-4cc3-9d7e-e83993e66ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we're never going to actually feed entire text into a Transformer all at once that would be computationally very expensive and prohibitive\n",
    "#so when we actually train a Transformer on a lot of these data sets we only work with chunks of the data set\n",
    "#and when we train the Transformer we basically sample random little chunks out of the training set and train on just chunks at a time\n",
    "#and train on just chunks at a time and these chunks have basically some kind of a length and some maximum length called block size.\n",
    "\n",
    "block_size =8                 #consider as examples\n",
    "train_data[:block_size+1]     #[0 to n-1] indexes   #n elements      \n",
    "#time-stamp 16:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b642ba-639e-49a2-8b5c-d66bb51a2387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]       \n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")\n",
    "\n",
    "#Okay so we've looked at the time dimension of the tensors that are going to be feeding into the Transformer\n",
    "#we're going to have many batches of multiple chunks of text that are all like stacked up in a single tensor\n",
    "#and that's just done for efficiency just so that we can keep the gpus busy because they are very good at parallel processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b4c87f-4257-42e1-bd3d-1fde7d903a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 76049, 234249, 934904, 560986])\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "#we just want to process multiple chunks all at the same time\n",
    "#but those chunks are processed completely independently they don't talk to each other\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "#random number generator so that the numbers I see here are going to be the same numbers you see later\n",
    "block_size= 8       #maximum context length for prediction\n",
    "batch_size= 4       #how many independent sequences will we process in parallel?\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))   #torch.randint(low,high,size)  #default low=0  #torch.randint(high,(m,n)) \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    #x, y = x.to(device), y.to(device)\n",
    "    print(ix)\n",
    "    return x,y\n",
    "\n",
    "xb,yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "for b in range(batch_size):        #batch dimension\n",
    "    for t in range(block_size):        #time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d37331b9-892d-4909-afe5-cae29bd253f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):   #class Child(Parent):   #When Child is created, it automatically gets everything from Parent.\n",
    "\n",
    "    def __init__(self, vocab_size):   \n",
    "    #__init__ is a special method in Python classes, also known as the constructor.\n",
    "    #It is automatically called when an instance (object) of a class is created.\n",
    "        \n",
    "        super().__init__()   #calls parent's class constructor i.e. nn.Module\n",
    "        #each token directly reads off the logits for the next token from lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        logits =  self.token_embedding_table(idx)  #(B,T,C) \n",
    "        #we're using an. embedding which is a very thin wrapper around\n",
    "        #basically a tensor of shape voap size by vocab size\n",
    "        #and what's happening here is that when we pass idx here\n",
    "        #every single integer in our input is going to refer to this embedding table\n",
    "        #and it's going to pluck out a row of that embedding table corresponding to its index\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape                   # this \n",
    "            logits = logits.view(B*T, C)             # handles the\n",
    "            targets = targets.view(B*T)              # issue\n",
    "            loss = F.cross_entropy(logits, targets)  #here is a issue       #check in documentation  #it wants B * C * T instead of BTC\n",
    "                                                 \n",
    "        return logits,loss  #basically the scores for the next character in sequence\n",
    "        #we are predicting what comes next based on just individual identity of a single token\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B,T) array of indices in the current context \n",
    "        for _ in range(max_new_tokens):\n",
    "            #get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:,-1,:]  #bocomes (B,C)\n",
    "            #apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  #(B,C)\n",
    "            #samples from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  #(B,1)\n",
    "            #append samples index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)  #concat\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)     #passing channel-D\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "#currently the tokens are not talking to each other and theyare notseeing any contxt except for they are just seeing themselves so i'm a token no. five and then i can actually make pretty decent predictions about what comes next just by knowing that i'm token five\n",
    "#because the some characters follow other characters n typical scenarios\n",
    "#loss -> quality of predictions\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# self parameter\n",
    "# self represents the instance of the class.\n",
    "# It allows access to variables and methods inside the class.\n",
    "\n",
    "# What is an Instance in Python?\n",
    "# An instance is an individual object created from a class.\n",
    "# Each instance has its own separate data,\n",
    "# even if multiple instances are created from the same class.\n",
    "\n",
    "#idx = torch.zeros((1,1), dtype = torch.long) #B,T\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long) , max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "975cf69a-8385-4b7a-9a6c-b0aaaa82365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)  #lr -> lowering rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d9cf50-bb67-4d62-9f85-a7946954330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 817891,  516880,  384662,  325959,  741921,  974304,  370067,  189337,\n",
      "         558787,  756870,  411115,  504091, 1002123,  840873,  912519,  517853,\n",
      "          55721,  703930,  203368,  315494,  660510,  441158,  709024,  977655,\n",
      "         295614,  212033,  796493,  823341,  999735,  736278,  342288,  816022])\n",
      "tensor([702318, 870079, 228655, 787149, 415707, 131813, 667408, 346477,  47758,\n",
      "        801178, 849330, 168712, 580282, 816119, 313461, 702536, 255986, 837246,\n",
      "        398759, 118964, 985630, 498505, 934202, 847840, 463562, 677529, 204322,\n",
      "        492817, 287133, 634923, 998396, 722269])\n",
      "tensor([ 82638, 787179, 954815, 865819, 187510, 663344,  10562, 276045, 352169,\n",
      "        423296, 591024, 740076, 524947, 285815, 434559, 808141, 813294, 294859,\n",
      "        583303, 914143, 565410, 899814, 857854,  73079, 389531, 667387, 780801,\n",
      "         24960, 946691, 803725, 385054, 291299])\n",
      "tensor([948169, 500116, 711213, 445470, 512047, 554571, 803763, 867377,  93542,\n",
      "         90291, 131643, 471535,  79619, 706734, 780285, 142829, 490017, 855239,\n",
      "        221556, 762898, 426680,  60953, 286553,  24018, 551822, 677691,  89721,\n",
      "        713099, 673862, 353159, 158271, 783695])\n",
      "tensor([984635, 614727, 900203, 777236, 782873,   1755,  35866,  82916, 641083,\n",
      "        444823,  29114, 428590, 949063, 423737, 432217, 845767, 161466,  99438,\n",
      "        580244, 137753, 370009, 914015, 648945, 244216, 543028, 280666, 216474,\n",
      "         73146, 712111, 436667, 222513, 702561])\n",
      "tensor([474962, 717383, 479741, 663612, 895823, 782617, 414272, 248818,  56278,\n",
      "        894729, 506353, 777353,  11576, 715381, 776162, 596981, 577791, 182971,\n",
      "        317969,  11245, 674514, 919646, 481254, 983053, 966432, 296779, 257148,\n",
      "        429147, 293478, 293998, 201802,  92774])\n",
      "tensor([610954, 990357, 364658, 802904, 696798, 868325, 412124, 699730, 986652,\n",
      "        517125, 462712, 387815, 448150, 923355, 722939, 174595,  23992, 820387,\n",
      "        703761, 915324, 897179, 899606,  13631, 707077, 471755, 484545, 480147,\n",
      "        452604, 155623, 364511, 531408, 417421])\n",
      "tensor([ 228222,  307152,  855485,   41359,  408121,  158054,  387178,  790303,\n",
      "         377490,  497504, 1001261,  875552,  279709,  614187,  981791,   39894,\n",
      "         134795,  258769,  368495,  294984,  748603,  930221,  815066,  214944,\n",
      "         779797,  406516,  495128,  410476,  793917,  421700,  536907,   83525])\n",
      "tensor([222077, 190304, 140664, 641268, 860876, 384184, 869862,  74277, 260777,\n",
      "        709929, 355275,   1858, 780970, 450041, 116160,  25160, 954623, 296401,\n",
      "        749634, 372379, 630010, 881869, 196451, 578343, 526415, 221274, 227944,\n",
      "        290347, 545674, 498814,  89356, 715834])\n",
      "tensor([982728, 867106, 440397, 336739, 390321,  74624, 712417, 869294, 109015,\n",
      "        386650, 518600,  75231, 871701, 490902, 402147, 765375, 118021, 918220,\n",
      "        525357, 757618, 302010, 161869, 921403, 671763, 269948, 523157, 963965,\n",
      "        926368, 470159,  32748, 338497, 407053])\n",
      "tensor([381011, 515704, 619332, 554960,  89028, 593215, 421624, 330150, 386948,\n",
      "        965980, 397314, 997764, 728810,  68250, 595254, 223203, 358096,  31281,\n",
      "        276444, 451699, 130370, 752162, 319557, 856009, 598002, 603812,  61157,\n",
      "        795734, 426508, 232092, 495907, 977283])\n",
      "tensor([331222, 380667, 837183, 179820,  23462, 142543, 261797, 985452, 121648,\n",
      "        457223, 667565, 525016, 693847, 812416,  97599, 394467, 136061, 322544,\n",
      "        338707, 508169, 239069, 462117,  50009, 471082,  27595, 316030, 297867,\n",
      "        853162, 599374, 661809, 907551, 576270])\n",
      "tensor([292407, 812065, 189768, 135162, 147577, 335379, 869832, 584824,  42704,\n",
      "        217047, 366835, 745600, 802443, 715821, 801102, 916998, 673645, 659029,\n",
      "         64793, 457322, 500235,  90720, 434820, 806962, 963318, 442334, 209215,\n",
      "        117014, 684355, 817153, 398986, 257497])\n",
      "tensor([269874, 389311, 517704, 763611,  13368, 164091, 416793, 625448, 623739,\n",
      "        261043, 586167, 121939, 261248, 344264, 127328, 340709, 361832, 389651,\n",
      "        985008, 584654, 927543, 984295, 957175, 497023, 344786, 537633, 461391,\n",
      "        418987, 539838, 744773, 428262, 954923])\n",
      "tensor([951637, 173583, 632506, 304165,  33677, 319373, 665550, 283813, 449785,\n",
      "        865794, 105013, 295988,   9224, 137476, 137768,  36738, 786624, 819202,\n",
      "        718012, 610270, 557119, 558048, 343331, 351437, 847111, 882658, 669766,\n",
      "        891104, 295646, 643992, 856931, 585740])\n",
      "tensor([ 72114, 504639, 958644, 533145, 690344, 273463, 542082, 223761, 715557,\n",
      "        798982, 291541, 304731, 241136, 940931, 677102, 290687, 194329, 370513,\n",
      "        969261, 156080, 641776, 397462,  87734, 300625, 543245, 292030, 815385,\n",
      "          9183, 668463, 526109, 191316, 719252])\n",
      "tensor([ 77802, 693069,  81977, 543909,  37769, 562027, 835159,  94611, 168050,\n",
      "        991677, 652561, 102817, 667200, 240592, 129377, 452442,  83297, 591665,\n",
      "        118801, 846219,  29746, 888502, 150417, 183665, 886499, 650412, 481187,\n",
      "        890253, 602923, 777767, 753606, 541408])\n",
      "tensor([627861, 355021,  20588, 345919, 580202, 600440, 586348, 788408,  34462,\n",
      "        347060, 968002, 658004, 500452, 804821, 461902, 958966, 340665, 510922,\n",
      "        914527, 521436, 468368, 165304, 133899, 548592,  72742, 852281, 225712,\n",
      "        425699,  74422, 595821, 618102, 339813])\n",
      "tensor([819062, 426627, 169594, 469953, 184311, 849785, 614836, 430243, 984675,\n",
      "        996766, 276352, 808920, 754899, 151104, 544491, 838123, 641850, 876277,\n",
      "        281845, 697142,   1767, 597960, 691145, 766493, 224669, 446675, 679281,\n",
      "        506805, 645808, 439127, 468287, 750731])\n",
      "tensor([333529, 632313, 127047, 616186, 128430,  74549,  71048,  28859, 384429,\n",
      "        525394, 213972,  34160, 651676, 258152, 600712, 969244, 946565, 756456,\n",
      "         54868, 159306, 416920, 771172, 517829, 979995,  22223, 772054, 722694,\n",
      "        717604, 704304, 478545, 433855, 443317])\n",
      "tensor([619534, 335996, 431999,   3898, 280587, 363499, 571936, 818556, 930204,\n",
      "        642211, 699312, 471439, 905790, 404971, 866885, 467664, 657214, 633762,\n",
      "        719371,  31668, 753016, 383396, 194237,  75775, 758220, 238553, 226334,\n",
      "        116635, 842770, 761962, 787164, 667586])\n",
      "tensor([ 54977, 182331, 268165, 380746, 880820, 861679, 186663, 370711, 310317,\n",
      "        344651, 720908, 647280, 484608, 351493, 446886, 134685, 499468, 181906,\n",
      "        306915, 803231, 846304, 683902, 817694, 858605, 626550, 690995, 514919,\n",
      "        365015, 333875, 435518, 572498, 871222])\n",
      "tensor([659245, 451273, 542890, 789374, 178793, 533876, 897402,  79466, 228507,\n",
      "        738093, 688322, 609622, 480610, 356335, 929711, 516046, 667163, 116664,\n",
      "        731769, 430602, 457537, 332279, 560733, 966001, 777244, 774892, 579825,\n",
      "        590878, 677638, 640278, 606337, 532163])\n",
      "tensor([683865, 272234, 616210, 880451, 236624, 373164, 171954, 345105,  12128,\n",
      "        525430,  93376,  16867, 426333, 980788, 907464, 989681, 677595, 977358,\n",
      "        896829, 304305, 529789, 696760, 120483, 134212, 632629, 905926, 196470,\n",
      "        809363, 935460, 464653, 915902, 537735])\n",
      "tensor([ 85720, 484164,  35825, 915003, 719822, 294768, 761112, 718113, 773821,\n",
      "        173437, 768560, 545671, 760180,  84761, 280097, 669599, 958330, 910746,\n",
      "        691827, 475583, 113587, 182192, 113401, 772303, 318933, 677878, 506548,\n",
      "        839495, 690703, 805909, 301467, 118410])\n",
      "tensor([205563, 713619, 626470, 967389, 358468, 542651, 517171, 456564, 431978,\n",
      "        632956, 671299, 144378,  54147, 734207, 958097, 397044, 432651, 268143,\n",
      "        216721,  72302, 581387, 391366, 965018, 686682, 780827, 650335, 248153,\n",
      "        207639, 728402, 657218, 489563, 654472])\n",
      "tensor([996158, 414275, 257040, 855494, 700415, 559099,  46979,  77592, 193152,\n",
      "        673008, 202520, 464353, 822732, 411026, 586763, 934985, 109228, 327477,\n",
      "         60201,  25630, 421812, 805283, 103054, 600889, 709684, 890057, 796948,\n",
      "        531549, 214259, 710727, 198569,  64038])\n",
      "tensor([622017, 960517,   8565, 637159, 466279, 704710, 413699, 592775, 937736,\n",
      "        816076, 962554, 431336, 729961, 411836, 527510, 245862, 184032, 460361,\n",
      "        443158, 433626, 993600, 115297, 765941, 367939, 102336, 843522,  91119,\n",
      "        132375, 900286, 883420, 298618,  32607])\n",
      "tensor([617010, 431996, 594321, 769275, 833438, 142228, 921211, 475912,  11938,\n",
      "         64460, 733228, 637034, 297893, 668909, 446616, 942135, 403364, 519538,\n",
      "        783426, 225691, 277401, 368079, 873755, 660711, 123530, 866079, 139866,\n",
      "        703200, 684328, 956531, 410376, 564119])\n",
      "tensor([843803, 489000, 454721, 499379, 441337, 884458, 494523, 824027, 336691,\n",
      "        992018, 342129, 397944, 660291, 721902, 248615, 585022, 907079,  25222,\n",
      "        107419, 309530, 272735, 561318, 806766, 670661, 358927, 627760, 725970,\n",
      "        796609, 461555, 921858,  60670, 852229])\n",
      "tensor([ 210818,  978879,   55563,   97879,  745399,  754049,  871047, 1002230,\n",
      "         356770,  732672,  411907,  483115,  844306,  551659,  958008,  646021,\n",
      "         379046,   16990,  976294,  825371,  877476,  836571,  565172,  360276,\n",
      "         262491,  708955,  140563,  716792,  346949,  199193,  512764,  362586])\n",
      "tensor([356944, 744771, 617042, 272294, 214445, 615107, 369584, 463350, 251178,\n",
      "        462769, 253576,  63606, 898593, 503472, 194049, 210961, 953022, 932430,\n",
      "        802779, 143745, 486306, 840526, 863317, 389878, 504530, 260746, 757204,\n",
      "        918651, 859619, 516317, 492408,  69068])\n",
      "tensor([ 65115, 960349, 883803, 815100, 191281, 209463, 566772, 191307, 697049,\n",
      "         97674, 852623, 483771, 963098, 357904, 771976, 770153, 375415, 563116,\n",
      "        975742, 511667,  94738, 215453, 262722,  22344, 983026,  47342, 786375,\n",
      "        565021, 533718, 120758,  24152, 428253])\n",
      "tensor([231387, 357640, 420246,  84570, 117458, 801269, 684715, 754442, 920108,\n",
      "        114987, 760279, 809778, 387575, 508961,  33062, 218075, 607899, 123568,\n",
      "        227500, 517894, 192719, 567004, 803253, 432884, 837378, 672961,  17832,\n",
      "        391102,  80642, 321007, 284017, 423455])\n",
      "tensor([867216, 792549, 798454, 863267, 951518, 521904, 504171, 316440, 895989,\n",
      "         55243, 897866, 215554, 718822, 374791,  72579,  28398, 986177, 781565,\n",
      "        326889, 484895, 640671, 628654, 495153, 361957, 569777, 704983, 465128,\n",
      "        420071, 482180,  83636, 722112, 509817])\n",
      "tensor([681841, 703559, 844488, 872858, 874740, 657626, 688194, 347153, 419402,\n",
      "        226937,  32738, 475015, 533641, 793307, 459985, 766259, 504260, 393669,\n",
      "         62188, 257225, 384226, 524239, 132513, 556929, 674264, 454450,  44762,\n",
      "        576036,  70838, 713009, 363425, 363088])\n",
      "tensor([558936, 804010, 754761, 298360, 494025, 268181, 339679, 755649, 843158,\n",
      "        656335, 122085, 724918, 102368, 910115, 499844, 999424, 348269, 764261,\n",
      "        713042, 196814, 292494, 309390, 514841, 430009, 208686, 392166, 601826,\n",
      "        920312, 238916,   3123, 636094, 792428])\n",
      "tensor([928524,  44056, 969758, 956272, 945724, 995992, 414133, 118667, 255034,\n",
      "        660710, 356503,  95018, 792166, 332039,  32884, 282376, 920464, 194896,\n",
      "        598832, 729061, 454508, 143751, 845067, 263181, 746720, 756618, 452558,\n",
      "        503488, 203330,  48395,  82067, 316557])\n",
      "tensor([131152, 790692, 280942, 839912, 903616, 364429, 319302, 133781, 891461,\n",
      "        437264, 362522, 285320, 345447, 469427, 143772, 559635, 976839,  10188,\n",
      "        678016,  97621,  83211, 994812, 177361, 254505, 152187, 388488, 186211,\n",
      "        300756, 400905, 496718, 980391, 824117])\n",
      "tensor([274742, 439580, 211043, 187408, 957463, 874277, 285493,  75822, 349875,\n",
      "         36511, 346045, 473774, 421718, 204637,  54766, 164969, 892940, 598317,\n",
      "        270422, 186203, 753436, 349324, 207600, 744010, 179582, 925045, 445620,\n",
      "        187801, 580919, 358756, 247492, 565467])\n",
      "tensor([335304, 716523, 119105, 633602, 258773,  54722, 345780, 738328, 949463,\n",
      "        827829, 868215, 935776, 572564, 543414, 329574, 834394, 282152, 375505,\n",
      "        173871, 813738, 130622, 240694, 620174, 233093, 444236, 741230, 323044,\n",
      "        370886, 107055, 728901, 376833,  35246])\n",
      "tensor([289884,  88936, 709089, 585757, 685447, 418812, 641603, 841769, 855038,\n",
      "        468094, 828050, 543280, 562744,  74697, 437046, 528284, 146143, 399859,\n",
      "        380429, 155781, 672114, 120623, 288133, 349944, 101598, 580237, 741887,\n",
      "        358342, 132895, 468713, 455909, 354113])\n",
      "tensor([772526, 398998,  16788, 622824, 709931,  78925, 457288, 364519, 611155,\n",
      "        138809, 946912,  65213, 950465, 176060, 737485, 679411, 733230, 462793,\n",
      "          6567, 394231, 728049,  19046,  76012,  11653, 992279, 103213, 325804,\n",
      "        551204, 624266, 144895, 901759,    743])\n",
      "tensor([714417, 490230, 416252, 396153,  65063, 325323, 604039, 901655, 457368,\n",
      "        306448, 237575,  98801, 784252,  30914,   3511, 718096,  13272, 641951,\n",
      "        524316, 926613, 477177, 588727, 642837, 420520, 799427,  73455, 548650,\n",
      "        996462, 943145, 237637, 838698, 460251])\n",
      "tensor([349232, 996324, 408152, 517413, 606512, 127920, 953807,  20292, 644454,\n",
      "        640224, 461149, 689713, 969382, 443276, 922593, 565532, 794218, 409930,\n",
      "        215958, 271011, 962176, 551901, 187271, 484118, 592527, 328605, 754728,\n",
      "         35957, 597860, 120121, 820750, 442790])\n",
      "tensor([619355, 713818, 470925, 896580, 415584, 596884, 772806, 810162, 565615,\n",
      "        357767, 392826, 976641, 457665, 407763, 953478,  58792, 966931, 579445,\n",
      "        430827,  83537,  74553, 930197, 944758, 274440, 759058, 837304,  11023,\n",
      "        115845,  93757, 645701, 313012, 465495])\n",
      "tensor([159190, 288028, 541419, 953803, 419633, 825362, 852520, 451004, 906270,\n",
      "        700685, 720662, 616690, 157792,  17487, 991522, 930403,  85155, 744686,\n",
      "        135005, 889571, 218711,  63015, 277286, 740857, 428176, 483366, 940565,\n",
      "        898897, 931465, 349177, 834513, 548283])\n",
      "tensor([312524, 535520, 409370, 679823, 999096, 947797, 771427, 929457, 774342,\n",
      "        905655, 814907, 387434, 587409, 381504, 735626, 590741, 421361, 394225,\n",
      "        534656, 257292, 314411, 129508, 707281, 424499, 664194, 751896, 458130,\n",
      "        398733,   4897, 864759, 829172, 270285])\n",
      "tensor([201340, 244873,  35010, 753702, 486667, 692805, 521350, 424594, 946248,\n",
      "        864014, 888300,  90733, 241114, 322101,  73311, 370681,  87036, 769486,\n",
      "        322325, 703635, 866388, 717128, 312759, 445181, 220652, 274001, 473042,\n",
      "        480679, 141264, 426131, 339528, 789196])\n",
      "tensor([879726, 148267, 255249, 404388, 706634, 982900, 575849, 993770, 362002,\n",
      "        263693, 640560, 474143,  57715, 156624, 923836,  60065, 898065, 604897,\n",
      "        989873,  87252, 575452, 974804,  60368, 274514, 624425, 800127, 446234,\n",
      "        306495, 586943, 980668, 386096, 552879])\n",
      "tensor([445947, 659691, 162362, 431361, 995178, 632467, 114925, 319284, 436882,\n",
      "        975761, 553569, 216080, 138509, 434868, 990856, 326815, 819695, 330455,\n",
      "        407374, 299175, 145884, 722733, 333053, 656874, 646306, 328517,  80212,\n",
      "        976641, 935460, 839340, 212615, 882840])\n",
      "tensor([ 13515, 626241, 313552, 783014, 977711, 459472, 476600, 214777, 195871,\n",
      "        434699, 663171, 229580, 630102, 756095, 220844, 852927, 452718, 320569,\n",
      "        736189, 922728,  87396, 472238, 492292, 931869,  84631, 357365, 931592,\n",
      "        181593, 964210, 779246, 730716, 512872])\n",
      "tensor([851517,  11263, 548669, 621768, 449862, 606285, 731798, 367593, 715489,\n",
      "        928875, 927686, 763645, 327083, 832995, 565050, 432192, 281390,  85395,\n",
      "         18031, 883112, 270070, 543209, 934431, 614352, 388431, 580133, 907626,\n",
      "        486128, 314386, 274759, 999264, 873841])\n",
      "tensor([646406, 271752,  20391, 199504, 106442, 578480, 772560, 913852,  38347,\n",
      "         11597, 507140, 287664, 877468, 160370, 726024, 838484, 943403, 287772,\n",
      "        260125, 814129, 769677, 667548, 384718, 156183,  80955, 633673, 670938,\n",
      "        595391, 572098, 347194,  21653, 464087])\n",
      "tensor([915435, 355149, 302925, 729417, 151056, 639936, 789009, 898110, 263011,\n",
      "        139770, 125326, 912967, 946393, 631550, 826661, 612791, 371458, 243504,\n",
      "        140573, 417751, 796819, 626191, 193126, 522155, 352162, 322382, 396430,\n",
      "        537739, 761494, 446189, 560870,   6552])\n",
      "tensor([ 866244,  412317,  780252, 1001156,  429177,  875873,  657140,  682178,\n",
      "         197476,  253680,  112964,  295382,  810886,  129623,  507738,  606663,\n",
      "         445335,   90165,  384011,  586375,  891585,  597451,  158866,   71250,\n",
      "         441674,  478828,  816334,   76487,  480963,  835142,  246960,  729003])\n",
      "tensor([160906, 582821, 813645, 297740,  39557, 220379, 167692, 705834, 337216,\n",
      "         89456, 321272, 757748, 200156, 564435, 446246, 725191,  47794, 704390,\n",
      "        362048, 735412, 721222, 642983, 392196, 509509, 931801, 354235, 844534,\n",
      "        800602, 120349, 995667,  36708, 291302])\n",
      "tensor([834829, 914679, 327054, 648866, 782563, 477469, 471294, 833089, 208503,\n",
      "         42062, 681118, 379379, 951346, 371130, 748955, 247017, 311684, 488401,\n",
      "        251367, 810541, 336712, 161826,  79884, 478079, 744631, 956005, 378935,\n",
      "        449005,  52244, 542259, 608887, 740570])\n",
      "tensor([695562, 517566, 783413, 291235, 950756, 706901, 981281, 798768, 335145,\n",
      "        169050, 101512, 694899, 686647, 390407, 675503, 593182, 131287, 747090,\n",
      "        339531, 720608, 375928, 565023, 800241, 589259, 922960,  55824, 304213,\n",
      "        163259, 646641,  58846, 726392, 365137])\n",
      "tensor([ 486410,  994194,   47746,  151014,  880213,  652631,  258538,  444836,\n",
      "          48655,  223011,  854920,   44865,  485298,  518583,  413693,  947566,\n",
      "         592210,  287872,  654746,  626281,  615636,  944792,  485839,  353693,\n",
      "         747510,  256970,  890768,  925644, 1003713,  199239,  840036,  465801])\n",
      "tensor([138636, 218811, 456167, 565342, 666133, 345884, 894826, 190938,  27992,\n",
      "        656306, 601446,  98808, 338476, 730926, 400227, 824618,  81726, 712104,\n",
      "        720822, 341441, 345691, 382953, 513421, 692107, 926483,  28311, 599360,\n",
      "        723912, 134280, 351718, 686236, 655029])\n",
      "tensor([205789, 425359, 269770, 501086, 734269, 788156, 725442, 679607, 710955,\n",
      "        967939, 589853, 764249, 709458, 870982, 261876, 383350, 316246, 535717,\n",
      "        561170, 948390, 725861, 340878, 974865, 139145, 361854, 138224, 635970,\n",
      "        167616, 992149, 409343, 273471, 288230])\n",
      "tensor([302037, 837470, 648384, 478402, 795624, 666881, 193445, 423833, 933190,\n",
      "        719329, 909994, 364228, 488592, 181650, 407141, 507002,  65351, 703964,\n",
      "        671250, 531105, 936686, 372922, 796648, 992821, 318476, 354002,  55368,\n",
      "        201850, 583627,  62468, 103314, 920106])\n",
      "tensor([705808, 952717, 130114, 383581,  83700, 764425, 863198, 739094,  74824,\n",
      "        514346, 975344, 574651, 548037,  24115, 803134, 161553, 147706, 853720,\n",
      "        556570, 745092, 772810, 266389, 518870, 372015, 611485, 840422, 258816,\n",
      "        878229, 537947, 104747, 664635, 691883])\n",
      "tensor([ 34231, 383071, 565478, 458750, 638986, 150053, 231967,  27373, 969203,\n",
      "        265975, 475796, 778413, 239848, 470115, 329564,  78328, 523360, 195183,\n",
      "        644261,  73787, 590048, 975433, 453066, 646867, 556437, 797383, 962202,\n",
      "        979687, 235945, 541364, 254305, 516678])\n",
      "tensor([946906, 846354,  19975, 162870, 280628, 205604, 469701, 792860, 537871,\n",
      "        401174, 636864, 584856, 706101, 716821, 214356, 559651, 953417,  67370,\n",
      "        980214, 508776, 902821, 554229, 217529,  61403, 863477,  24893, 649938,\n",
      "        701963, 113199, 220078, 740053, 874099])\n",
      "tensor([173144, 547577,  57915, 917652,  19469,  24212, 514392, 473671, 552107,\n",
      "        401020, 928913, 647847, 998817,  88642, 192331, 241399, 886146, 878992,\n",
      "         93179, 103703, 996699, 531656, 833005, 968042, 474583, 975194, 103472,\n",
      "        605925, 988560, 642021, 647273, 215257])\n",
      "tensor([120201, 124906, 282892, 899497,  31581, 506461, 924745, 464699, 425443,\n",
      "        820005, 665379, 753320, 759982, 267651, 261866, 261434,  23195, 243326,\n",
      "        411768, 976820, 524013, 473626, 919515, 800669, 327901, 321859, 415951,\n",
      "        198514, 682949, 917190, 139774, 875196])\n",
      "tensor([609222, 528489, 147073, 442812,  19569, 528249, 487654, 689872,  35243,\n",
      "        769087, 188819, 957672, 436394, 472526, 673563, 662961, 492097, 900566,\n",
      "        335083, 777978, 213755,  55738, 417617,  38012, 557323, 440951,  86304,\n",
      "        863151, 903232,   6171, 815347, 763059])\n",
      "tensor([385889, 144656, 465838, 640833, 785028, 741681, 694937, 456083, 270380,\n",
      "        285616, 559641, 522346, 664978,  84764, 828232, 993389, 254306, 394223,\n",
      "        945801, 733235,  70886, 993595,  74561, 284127,  44353,  96895, 753907,\n",
      "        165425, 415037, 115093, 864446, 812824])\n",
      "tensor([ 748404,  430655,   24649,  738863,  536485,  569371,  298322,  496474,\n",
      "         472904,  359499,   89368, 1003213,  239146,  593028,  953556,  386784,\n",
      "         165112,  285547,  434700,  569755,  271407,  798022,  680401,  874545,\n",
      "         443320,  127151,  107515,  315447,  124852,   64242,  269147,  530665])\n",
      "tensor([324625, 102740, 218536, 366299, 116320, 406602, 502072, 968938, 149571,\n",
      "        388738, 565768, 678266, 975554, 491606, 509846, 172965, 776860, 665161,\n",
      "        996809, 841865, 890034, 750232, 995507, 872501, 575235, 977835, 592609,\n",
      "        991664, 386816, 424349, 549001,  57418])\n",
      "tensor([773417, 229994,  67671, 456719, 236184, 717210, 706506, 649519, 693890,\n",
      "        763799, 243640, 326623,  79988, 355483, 196829, 954577, 168956, 792443,\n",
      "        782346, 673540, 151756, 194306, 390330, 298016, 517331, 154977, 493509,\n",
      "        426666, 652973, 613074, 312182, 489969])\n",
      "tensor([824978, 692323, 881660, 649039, 938747, 718591, 995438, 776745, 233708,\n",
      "        442504, 538456, 210973, 387578, 251453, 198693, 130783, 827031, 134323,\n",
      "        165957, 793698, 742866, 495633, 214301, 986319, 658402, 161880, 130578,\n",
      "        608903, 200293, 154212, 622100, 964885])\n",
      "tensor([697271, 144364, 254657,   5059, 510037, 183058, 525460, 310590, 770941,\n",
      "        248197, 202182, 544733, 982133, 980775, 748815,  55470, 562188, 771126,\n",
      "        378327, 182799, 734859, 306881, 607706, 528113, 324432, 753309,  98760,\n",
      "        631791, 819515, 109266, 529791,  24182])\n",
      "tensor([982533, 435563, 343603, 640204, 183956, 419954, 358502, 603051,  33085,\n",
      "         36085, 986720, 702823,  16058, 252208, 350665, 876730, 256611, 842779,\n",
      "        313209,  90456,  32795, 532285,  99171, 654042, 385042, 963204, 334940,\n",
      "        790223, 276663, 264737, 220650, 912015])\n",
      "tensor([823523, 486276, 402788, 193566, 574829, 224000, 553478, 769976, 656691,\n",
      "        550873, 658238, 494105, 216078, 741690, 594243, 638765, 919540, 170008,\n",
      "        733517, 773805, 360848, 395778, 135157, 710395, 299006, 450433, 967248,\n",
      "        734523, 604420, 306152, 897385, 742064])\n",
      "tensor([333445, 514416, 318175, 351033, 599885, 885802, 184903, 549004, 638706,\n",
      "        402240, 356780, 476885, 939236, 236150, 869492, 660708, 103013, 572669,\n",
      "        483430, 176542, 767284,   7309, 238378, 203070, 101492, 605904, 600965,\n",
      "        118372, 876169, 335071, 921083, 611551])\n",
      "tensor([832912, 269959,  99934, 541831, 260674, 452382, 872112, 272191, 517833,\n",
      "        135886,   2848,  60495, 992789,  46747, 695362, 431163,  84745, 379202,\n",
      "        835932,  71472,  90068, 140472, 391502, 135472, 391002, 525441, 440065,\n",
      "         76698,  17231, 499027, 495378, 264291])\n",
      "tensor([267284, 247297, 278221, 908125, 231295, 172914, 405373, 997485, 398460,\n",
      "        112902, 227498, 427122, 310663, 583701, 527812, 497801, 286057, 368666,\n",
      "        745420, 342974, 465808, 264565, 572255, 531818, 256455, 221637, 897484,\n",
      "        613466, 666568, 333771, 319710, 503862])\n",
      "tensor([657315, 516583, 245666, 751100, 461883, 173480, 905770, 153473, 818410,\n",
      "        565480, 138887, 654450, 215085, 210360, 356436, 835683,  85584,  52317,\n",
      "        632334, 512159, 470253,  75199, 544161, 807241, 995242, 860687, 115776,\n",
      "        140403, 598657, 684180, 331339,  83997])\n",
      "tensor([176902, 516473, 403576, 894451,  21868, 989994, 704959, 818198, 963521,\n",
      "        723950, 682155, 684051, 267867, 231432, 203404, 982697, 162180, 890714,\n",
      "        798589, 962898, 514655, 640017, 350128, 546775, 446005, 198458, 206670,\n",
      "         38687, 327498, 842812, 317349, 655656])\n",
      "tensor([259729, 986328, 507700,  22111,  65346, 988105, 124800, 785606, 410428,\n",
      "        647155, 563160, 796670, 528243, 883915, 997495, 966265, 582284, 102688,\n",
      "        766569, 724425, 235799,  85688,  66011, 452213, 326924, 383027, 531408,\n",
      "        545079, 958988, 834293, 629880, 822981])\n",
      "tensor([962071, 965997, 236010, 854280, 940963,  24114, 924430, 760868,  43219,\n",
      "        622540, 266452, 102339,  96954, 410744, 600299, 396429, 910936, 133147,\n",
      "        709046, 171179, 876837, 785882, 473580, 633115, 794073, 935451, 143342,\n",
      "        399349, 335622,   4410, 742955, 786923])\n",
      "tensor([808793, 642737, 940780, 629533, 953647, 455971,  85469, 295408,  52251,\n",
      "        512358,  14968, 470465, 679345, 208869, 612916, 252067, 332416, 142357,\n",
      "        841907, 143795, 930362,   2956, 115251, 224763, 631314, 731538, 418844,\n",
      "        154139, 921422, 281537, 822873, 106853])\n",
      "tensor([369811, 853904, 511363, 365222, 823762, 303039, 779857, 158173, 823901,\n",
      "        552148, 187920, 488013, 776576,   7823, 159333, 795354, 427919, 261809,\n",
      "        298160, 716242, 619842, 570197, 973543, 222759, 912907, 139514, 353321,\n",
      "        962540,  46731, 727808, 556048, 930073])\n",
      "tensor([ 557364,  294507,  685507,  694682,  889481,  115506,  429786,  689768,\n",
      "         361099,  795462,  565866,  348900,  298939,  730056,  625440,  141167,\n",
      "         447844,  111999,  287514,  963941,  630982,  972621,   52255,  264939,\n",
      "        1001690,  149677,  184535,  134763,  334233,  646000,  938524,  861987])\n",
      "tensor([279158, 688166, 428017, 535091, 640469, 279399, 945520, 429007, 560238,\n",
      "        902766, 665306, 143742, 625005, 602217, 779539, 496911, 543395, 632706,\n",
      "        505895, 967915, 112675, 207231, 534328, 258927, 254314, 461758,  95218,\n",
      "        993033, 472200, 625451, 761124, 606454])\n",
      "tensor([817926, 310455, 638653, 195385, 376795, 212347, 307560, 611500, 246242,\n",
      "        569685, 495699,  17905, 369663,  31783, 330354, 856915,    229, 120007,\n",
      "        581488, 308028, 396419, 870651, 372019, 169797, 638775, 480818, 494977,\n",
      "         23624, 606942, 648657,  97972, 599343])\n",
      "tensor([ 848144,  663084,  542691,  756275,  809052,  867548,  349919,   66420,\n",
      "          87454,  256254,   49019, 1002918,  503260,  284774,  818294,   61251,\n",
      "         212916,  550194,  590241,  159250,   97631,  496844,  384616,   28763,\n",
      "         352293,   70113,  435274,  807807,   10929,  727882,  125109,  446837])\n",
      "tensor([ 29039,  31295, 797083, 165228, 485223, 826260, 996853, 763564, 426748,\n",
      "         70479, 252730, 404289, 713045, 267767, 466889, 641955, 367859, 710084,\n",
      "        167733, 237257, 413473,  86431, 234796, 225052, 771718, 481273, 181642,\n",
      "        370604, 145131, 696580, 395715, 668903])\n",
      "tensor([ 637278,   33508,  450492,  201848,  488616,  707608,  358229,  716557,\n",
      "         788296,  916508,  555613,  832539,  417992,  609752, 1000499,  258280,\n",
      "         348403,  968003,  758265,  203321,  349244,  412640,  617632,  857786,\n",
      "          79975,  745589,  735586,  222558,   12804,  241019,  883852,  173296])\n",
      "tensor([ 710866,  459157,  736935,  702905,  438134,  839272,  292446,  686937,\n",
      "         210694,   54357,  152082,  609270,  753356,  289880,  577161,  267565,\n",
      "         355706,  169642,  664017,  326244,  247062,  667501,  363972, 1002233,\n",
      "         645170,  581509,  585871,  121539,  284925,  570569,  551029,  625998])\n",
      "tensor([457314, 431022, 924352, 638062, 655274, 991731,  58082, 367374, 918513,\n",
      "        788710, 686397, 912934, 519986, 299335, 643095, 793663, 403302, 156793,\n",
      "        149034, 172371, 149147,  77289, 346486, 815258,  22330, 314530, 533058,\n",
      "        856716, 335059, 615716, 218147, 694416])\n",
      "tensor([776440, 939649, 237769, 238306, 676215, 256328, 899493, 774947, 405253,\n",
      "        127397,  80922, 333251,  87381, 563632, 216406,  90729, 438162,  10163,\n",
      "         79739, 242706,  86374, 106939, 413635, 660528, 732299, 289850, 575345,\n",
      "        544605, 776765, 823419, 778522, 534433])\n",
      "tensor([197794, 455925, 332858, 295733, 498776, 211774, 563333, 222304,  38370,\n",
      "        627909, 444126, 901192, 428383, 307021, 802898, 234786, 780525, 367760,\n",
      "        810740,  27526, 110686, 936963, 851320, 771328, 239790, 215164, 822693,\n",
      "        480647, 206229, 745603,  86156, 786149])\n",
      "tensor([412965, 897418, 644709, 578789, 341410, 183311, 241935, 875157,  92003,\n",
      "        716400,  56038, 497325, 232677, 564119, 905548, 580042, 805264, 721552,\n",
      "        174993, 630698, 931765, 491312, 182809, 680015, 428835, 832228, 804519,\n",
      "        610970,  95771, 144506, 906585, 898020])\n",
      "tensor([163874, 538020,  26842,  75250, 941425, 209315,  29971, 955049, 521832,\n",
      "        288634, 975018, 580808, 441732, 635254, 792799, 580570, 110144, 801434,\n",
      "        604461, 874669, 411614, 171419, 992014, 297999, 227228, 170295, 405452,\n",
      "        453557, 179656, 632156,   5173, 624978])\n",
      "tensor([712474, 146452, 174605, 980595, 847532, 933525, 355363, 906125,  47403,\n",
      "        793916,  84440, 751384, 530990, 284057, 722749, 315400, 499272,  74794,\n",
      "        706965, 638121, 559437, 286846, 466581, 384916, 682141, 330768, 130288,\n",
      "        242291, 284206, 685411, 294302, 518583])\n",
      "tensor([ 360821,  987112,  124574,  877027,  850429,  480931,  498619,  232395,\n",
      "         877730,  435937,  241703,  356667,  909916,  975691,  291183,  880535,\n",
      "        1000028,  660360,  318098,  619616,  133298,  948608,  646076,  165684,\n",
      "         697305,  972382,  849602,  770279,  804918,  163030,  715687,  642969])\n",
      "4.65630578994751\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 \n",
    "for steps in range(100):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb,yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d78e77f1-ae18-40d1-9685-ca05b671bdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "oTo.JUZ!!zqe!\n",
      "xBP qbs$Gy'AcOmrLwwt\n",
      "p$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPi\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long) , max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8fa9c2-3050-45a5-a54c-0fe6b18d3658",
   "metadata": {},
   "source": [
    "# The Mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c235d61c-b9e1-4d4f-b406-3ead07794fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n",
    "\n",
    "#token on the 5th location should not talk to 6,7,8,..., it will only talk to 4,3,2,...\n",
    "#it onnly predict from previous context\n",
    "#easiest way to token communicate is average of all the preceding elements #very lossy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af29f225-7603-474b-9782-1f78801cd937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))   #bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]  #(t,C)\n",
    "        xbow[b,t] = torch.mean(xprev,0) \n",
    "\n",
    "print(x[0])\n",
    "xbow[0]\n",
    "#very inefficeint\n",
    "#the trick is doing matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae1d6e63-b2b3-4021-81c1-168f7a255219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 2\n",
    "wei2 = torch.tril(torch.ones(T,T))\n",
    "wei2 = wei2 / wei2.sum(1,keepdim=True)\n",
    "xbow2 = wei2 @ x # (B,T,T) @ (B,T,C)\n",
    "torch.allclose(xbow, xbow2)\n",
    "wei2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39166ce4-9329-49f4-a751-33fa1d220a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xbow[0] , xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "104145a2-0664-4b62-b727-2fad04c86e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 3 : softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei3 = torch.zeros((T,T))\n",
    "wei3 = wei3.masked_fill(tril == 0 , float('-inf')) #all the element where tril is zero make it -infinity\n",
    "wei3 = F.softmax(wei3, dim=-1)\n",
    "xbow3 = wei3 @ x\n",
    "torch.allclose(xbow2,xbow3)\n",
    "#should be True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26353981-715a-4031-b9cc-e41e0e24759f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 4 : self attention\n",
    "#attention is a communication mechanism , number of nodes directed a graph, every node has vector of information\n",
    "# , it aggregate information via a weighted sum from all of the nodes that point to it\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "#let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)        \n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B,T,16)         #what other tokens should “look for”.\n",
    "q = query(x) # (B,T,16)        #what this token is “asking” for.\n",
    "wei = q @ k.transpose(-2,-1)   # (B,T,16) @ (B,16,T) ----> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril ==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x   #x is like a private information to this token\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c228bf78-5c60-4a06-9878-fc969b3d0caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0] \n",
    "#for the last row it was 8th token \n",
    "#it knows what content it has and it knows at what position it's in\n",
    "#now this token based on that creates a query (hi i am looking for this kind of stuff)\n",
    "#and then all nodes get to emit keys \n",
    "\n",
    "#may be one of the consonant and i am in a position up to 4 and that key would have high number in that in specific channel\n",
    "# query and key when they dot product they can find each other and create a high affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c008e61-8f86-4d19-ad1d-072201a923ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "___\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "__\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "#a = torch.ones(3,3)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)  #here 1 is dimention\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('___')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('__')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef8f995d-8fbc-4c63-b66c-f9e886c205fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multihead attention\n",
    "#simply by creating multiple heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52effd7-d02a-4653-a4d2-46f629ef3da9",
   "metadata": {},
   "source": [
    "Great question!\n",
    "\n",
    "Let’s break it down clearly — especially around this line:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "xbow2 = wei2 @ x  # (B,T,T) @ (B,T,C)\n",
    "You're asking how wei2 can be (B, T, T) — let's understand what’s really going on.\n",
    "\n",
    "🔹 1. What is wei2?\n",
    "python\n",
    "Copy code\n",
    "wei2 = torch.tril(torch.ones(T, T))  # shape: (T, T)\n",
    "wei2 = wei2 / wei2.sum(1, keepdim=True)\n",
    "So wei2 is a (T, T) lower triangular matrix where:\n",
    "\n",
    "Each row i contains equal weights for positions 0 to i\n",
    "\n",
    "Rows are normalized (sum to 1)\n",
    "\n",
    "Example (if T = 4):\n",
    "\n",
    "csharp\n",
    "Copy code\n",
    "tensor([\n",
    " [1.0000, 0.0000, 0.0000, 0.0000],\n",
    " [0.5000, 0.5000, 0.0000, 0.0000],\n",
    " [0.3333, 0.3333, 0.3333, 0.0000],\n",
    " [0.2500, 0.2500, 0.2500, 0.2500]\n",
    "])\n",
    "🔹 2. What is x?\n",
    "python\n",
    "Copy code\n",
    "x = torch.randn(B, T, C)\n",
    "This is your input sequence for B batches, each of length T, with C features per token.\n",
    "\n",
    "So:\n",
    "\n",
    "Shape of x = (B, T, C)\n",
    "\n",
    "🔹 3. How does the matrix multiplication work?\n",
    "Now this is where your confusion lies:\n",
    "\n",
    "You're doing:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "xbow2 = wei2 @ x\n",
    "But wei2 is only (T, T) — not (B, T, T).\n",
    "\n",
    "So how does PyTorch do this? It broadcasts the matrix multiplication across batches.\n",
    "\n",
    "Effectively:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "xbow2 = torch.matmul(wei2, x)  # (T,T) @ (B,T,C) => broadcasts to (B,T,T) @ (B,T,C)\n",
    "In math:\n",
    "\n",
    "For each sample b in batch B, it applies the same wei2 matrix:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "xbow2[b] = wei2 @ x[b]\n",
    "So:\n",
    "\n",
    "Shape of wei2: (T, T)\n",
    "\n",
    "Shape of x[b]: (T, C)\n",
    "\n",
    "Output for each b: (T, C)\n",
    "\n",
    "Finally, xbow2: (B, T, C)\n",
    "\n",
    "🔹 So why say “(B,T,T) @ (B,T,C)”?\n",
    "This is pseudo math notation.\n",
    "\n",
    "It means:\n",
    "\n",
    "We're broadcasting a (T, T) matrix multiplication across batch dimension\n",
    "\n",
    "Each batch does (T, T) @ (T, C), resulting in (T, C)\n",
    "\n",
    "Combined: (B, T, T) @ (B, T, C) → (B, T, C)\n",
    "\n",
    "But technically, wei2 is still (T, T), not (B, T, T). If you wanted different attention per batch, you'd manually make wei2 shaped (B, T, T).\n",
    "\n",
    "🔹 Summary\n",
    "wei2 is (T, T), and applied across batches\n",
    "\n",
    "x is (B, T, C)\n",
    "\n",
    "Result: xbow2 = (B, T, C)\n",
    "\n",
    "(B,T,T) @ (B,T,C) is conceptual shorthand — not literal unless you expand wei2 per batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c39278-ec52-45cf-89b8-b21caaa7e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0c38446-68c3-420b-9431-581b795ff3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/zer0/anaconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zer0/anaconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zer0/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zer0/anaconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zer0/anaconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "import requests\n",
    "urls = [\n",
    "    \"https://www.gutenberg.org/files/1342/1342-0.txt\",  # Pride & Prejudice\n",
    "    \"https://www.gutenberg.org/files/1661/1661-0.txt\",  # Moby Dick\n",
    "    \"https://www.gutenberg.org/files/84/84-0.txt\",      # Frankenstein\n",
    "]\n",
    "\n",
    "with open(\"tokenize_text.txt\", \"w\", encoding=\"utf‑8\") as fout:\n",
    "    for u in urls:\n",
    "        r = requests.get(u); r.raise_for_status()\n",
    "        t = r.text\n",
    "        s = t.find(\"Chapter 1\") if \"Chapter 1\" in t else 0\n",
    "        e = t.rfind(\"End of the Project Gutenberg\") or len(t)\n",
    "        fout.write(t[s:e].strip() + \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86b9ec5c-bd12-4ec5-944b-211d6d0e4bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24M parameters\n",
      "step 0: train loss 5.7797, val loss 5.7721\n",
      "step 100: train loss 3.4301, val loss 3.4258\n",
      "step 200: train loss 3.0588, val loss 3.0541\n",
      "step 300: train loss 2.9435, val loss 2.9427\n",
      "step 400: train loss 2.8494, val loss 2.8441\n",
      "step 500: train loss 2.7834, val loss 2.7915\n",
      "step 600: train loss 2.7265, val loss 2.7366\n",
      "step 700: train loss 2.6785, val loss 2.7008\n",
      "step 800: train loss 2.6186, val loss 2.6511\n",
      "step 900: train loss 2.5777, val loss 2.6330\n",
      "step 1000: train loss 2.5406, val loss 2.5911\n",
      "step 1100: train loss 2.5072, val loss 2.5673\n",
      "step 1200: train loss 2.4595, val loss 2.5246\n",
      "step 1300: train loss 2.4452, val loss 2.5127\n",
      "step 1400: train loss 2.3992, val loss 2.4986\n",
      "step 1500: train loss 2.3821, val loss 2.4568\n",
      "step 1600: train loss 2.3491, val loss 2.4571\n",
      "step 1700: train loss 2.3201, val loss 2.4266\n",
      "step 1800: train loss 2.2980, val loss 2.4250\n",
      "step 1900: train loss 2.2875, val loss 2.4244\n",
      "step 2000: train loss 2.2698, val loss 2.3930\n",
      "step 2100: train loss 2.2579, val loss 2.3858\n",
      "step 2200: train loss 2.2373, val loss 2.3836\n",
      "step 2300: train loss 2.2203, val loss 2.3720\n",
      "step 2400: train loss 2.1994, val loss 2.3574\n",
      "step 2500: train loss 2.1810, val loss 2.3366\n",
      "step 2600: train loss 2.1719, val loss 2.3243\n",
      "step 2700: train loss 2.1852, val loss 2.3399\n",
      "step 2800: train loss 2.1528, val loss 2.3381\n",
      "step 2900: train loss 2.1462, val loss 2.3184\n",
      "step 3000: train loss 2.1217, val loss 2.3227\n",
      "step 3100: train loss 2.1249, val loss 2.2949\n",
      "step 3200: train loss 2.1140, val loss 2.2852\n",
      "step 3300: train loss 2.1095, val loss 2.2988\n",
      "step 3400: train loss 2.0984, val loss 2.2635\n",
      "step 3500: train loss 2.0979, val loss 2.2754\n",
      "step 3600: train loss 2.0766, val loss 2.2671\n",
      "step 3700: train loss 2.0739, val loss 2.2759\n",
      "step 3800: train loss 2.0784, val loss 2.2398\n",
      "step 3900: train loss 2.0680, val loss 2.2389\n",
      "step 4000: train loss 2.0468, val loss 2.2428\n",
      "step 4100: train loss 2.0443, val loss 2.2412\n",
      "step 4200: train loss 2.0358, val loss 2.2199\n",
      "step 4300: train loss 2.0396, val loss 2.2351\n",
      "step 4400: train loss 2.0266, val loss 2.2307\n",
      "step 4500: train loss 2.0399, val loss 2.2187\n",
      "step 4600: train loss 2.0267, val loss 2.2104\n",
      "step 4700: train loss 2.0203, val loss 2.2134\n",
      "step 4800: train loss 2.0035, val loss 2.2105\n",
      "step 4900: train loss 2.0074, val loss 2.2014\n",
      "\u0000ET:\n",
      "There ceat more to lords, or my lrove\n",
      "Apomitanio, kingly uncly row rehap 'out?\n",
      "\n",
      "REMIOPETEO:\n",
      "O the hearth, sir.\n",
      "\n",
      "KING EDWARD II:\n",
      "Who care; thou one not look king,\n",
      "Clorend ashulb. O this blood, broad in thinks\n",
      "My march, love inst: I even deathen them;\n",
      "Moubty! shall polies and\n",
      "And, anothing hereof than his souling\n",
      "With Lanerphess my lord's of lords.\n",
      "\n",
      "MONTAGUE:\n",
      "Stand imbretily of overiling and more neart of a aill.\n",
      "You mare a villing, brother; how shole:\n",
      "and hath spysign to thie,\n",
      "Thus? bellow wheld to I thou shalt sind wrie, that the\n",
      "is Clarencil'd! wright my.\n",
      "\n",
      "GREMIO:\n",
      "You the root, reason; both powers are I de\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Build BPE tokenizer from tokenize_text.txt\n",
    "with open('tokenize_text.txt', 'r', encoding='utf-8') as f:\n",
    "    raw = f.read()\n",
    "tokens = list(raw.encode('utf-8'))\n",
    "\n",
    "def get_stats(ids):\n",
    "    stats = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        stats[pair] = stats.get(pair, 0) + 1\n",
    "    return stats\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# Build vocab\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "\n",
    "vocab = {i: bytes([i]) for i in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def bpe_encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "def bpe_decode(ids):\n",
    "    return b''.join(vocab[i] for i in ids).decode('utf-8', errors='replace')\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Load model training data from input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "data = torch.tensor(bpe_encode(text), dtype=torch.long)\n",
    "train_data = data[:int(0.9 * len(data))]\n",
    "val_data = data[int(0.9 * len(data)):]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Batch Sampling\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Transformer Model\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        B, T, C = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Training\n",
    "model = BigramLanguageModel().to(device)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Generate\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "output = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "print(bpe_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "662f7161-d2f5-4c55-b11a-23b5455a3a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 100277\n",
      "Parameters: 13.137077 M\n",
      "step 0: train 11.6588, val 11.6651\n",
      "step 100: train 7.0855, val 7.3425\n",
      "step 200: train 6.8948, val 7.1850\n",
      "step 300: train 6.4718, val 6.8266\n",
      "step 400: train 6.0932, val 6.5328\n",
      "step 500: train 5.8444, val 6.3606\n",
      "step 600: train 5.6367, val 6.1753\n",
      "step 700: train 5.4945, val 6.1046\n",
      "step 800: train 5.3693, val 6.0561\n",
      "step 900: train 5.2936, val 5.9905\n",
      "step 1000: train 5.1863, val 5.8829\n",
      "step 1100: train 5.1238, val 5.8325\n",
      "step 1200: train 5.0448, val 5.8389\n",
      "step 1300: train 5.0280, val 5.8149\n",
      "step 1400: train 4.9611, val 5.8013\n",
      "step 1500: train 4.9134, val 5.7779\n",
      "step 1600: train 4.8562, val 5.7175\n",
      "step 1700: train 4.7909, val 5.7194\n",
      "step 1800: train 4.7552, val 5.7266\n",
      "step 1900: train 4.7443, val 5.6973\n",
      "step 2000: train 4.7129, val 5.7034\n",
      "step 2100: train 4.6502, val 5.7071\n",
      "step 2200: train 4.6442, val 5.7201\n",
      "step 2300: train 4.6165, val 5.6896\n",
      "step 2400: train 4.6086, val 5.6944\n",
      "step 2500: train 4.5452, val 5.7400\n",
      "step 2600: train 4.5356, val 5.7106\n",
      "step 2700: train 4.4888, val 5.7393\n",
      "step 2800: train 4.4449, val 5.6976\n",
      "step 2900: train 4.4481, val 5.7021\n",
      "step 3000: train 4.4171, val 5.7201\n",
      "step 3100: train 4.4097, val 5.6737\n",
      "step 3200: train 4.3900, val 5.6980\n",
      "step 3300: train 4.3538, val 5.6697\n",
      "step 3400: train 4.3361, val 5.7350\n",
      "step 3500: train 4.3177, val 5.7277\n",
      "step 3600: train 4.3212, val 5.7225\n",
      "step 3700: train 4.2846, val 5.7438\n",
      "step 3800: train 4.2752, val 5.7707\n",
      "step 3900: train 4.2568, val 5.7734\n",
      "step 4000: train 4.2245, val 5.7806\n",
      "step 4100: train 4.2062, val 5.7305\n",
      "step 4200: train 4.1851, val 5.7467\n",
      "step 4300: train 4.1838, val 5.7625\n",
      "step 4400: train 4.1555, val 5.7463\n",
      "step 4500: train 4.1429, val 5.7565\n",
      "step 4600: train 4.1501, val 5.8083\n",
      "step 4700: train 4.1257, val 5.7493\n",
      "step 4800: train 4.0945, val 5.8367\n",
      "step 4900: train 4.0949, val 5.7883\n",
      "Generated:\n",
      " ! dreadful stir\n",
      "craft, give them here for your ear.\n",
      "\n",
      "DUKE OF YORK:\n",
      "As I could be satisfied to hence.\n",
      "\n",
      "Nurse:\n",
      "He answersoever his stay with her while; that die\n",
      "But yet they had rather desire on at a list.\n",
      "\n",
      "LEONTES:\n",
      "Would I see my advancement?\n",
      "\n",
      "LORD:\n",
      "I thought not instantlyMarry, good morath some, which\n",
      "I heard not patiently be now in att season\n",
      "No, brought heed changes to make pros--his\n",
      "To pluck him to not year to the shade.\n",
      "3 KING HENRY VI\n",
      "\n",
      "JULIET:\n",
      "And, geese her!\n",
      "O, she should not grow!\n",
      "Thusake me, come then, not not call'd\n",
      "Mis my pity and so fond Menen,\n",
      "And witness unchanging my lord.\n",
      "\n",
      "PAGE:\n",
      "I loved your exiled conceive: you, the\n",
      "odemon, I believe my man: manybrace\n",
      "What. Let me in him, come forth.\n",
      "\n",
      "ISABELLA:\n",
      "Bship!\n",
      "Patience you tell me for this friar, royal, you\n",
      "fionmen, and they contend!\n",
      "Welcome, here more's death, Claudio, sir,\n",
      "We not the word by his sword upon your stoned;\n",
      "Rosal, I will stay;\n",
      "He cannot make thee forget.\n",
      "Lo, most welcome, my Lord beard!\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Call that thou dost stay, let him be known.\n",
      "\n",
      "KATHARINA:\n",
      "You the prince 'ifs love not.\n",
      "\n",
      "COMINIUS:\n",
      "Have you play you. This on time come with one\n",
      "me to the great man I. Alunting.\n",
      "\n",
      "First Murderer:\n",
      "Give me to charge to his death.\n",
      "\n",
      "LUCIO:\n",
      "In power on the housewife of the plain springs of opposed\n",
      "By our extremes of the proclamation, not dead,\n",
      "If you fam gratis you' the cordsens crown into the\n",
      "I never with the people and to the moon,\n",
      "Nothing's a match consent of trample\n",
      "Had to Richmond, who we for his pousenced him;\n",
      "For who fitting is my wife of this great knightly\n",
      "Ere to live.\n",
      "\n",
      "LUCIO:\n",
      "And with a prince against a traitor\n",
      "With worms-govern'd our right, in most falsely tapster's ashes.\n",
      "\n",
      "QUEEN:\n",
      "O, brother Montague to jest;\n",
      "Romeoebus, bright stuff, the curbs music,\n",
      "When I heaven make me from me, and yet\n",
      "slaant, especially '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# ▼ Step 1: Download public domain for tokenize_text.txt\n",
    "# urls = [\"https://www.gutenberg.org/files/1342/1342-0.txt\"]\n",
    "# texts = []\n",
    "# for u in urls:\n",
    "#     with urlopen(u) as resp:\n",
    "#         txt = resp.read().decode('utf-8', errors='replace')\n",
    "#     start = txt.find(\"Chapter 1\")\n",
    "#     end = txt.rfind(\"End of the Project Gutenberg\")\n",
    "#     texts.append(txt[start:end].strip())\n",
    "# full_toktext = \"\\n\\n\".join(texts)\n",
    "# with open(\"tokenize_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(full_toktext)\n",
    "# print(\"Saved tokenize_text.txt, length:\", len(full_toktext))\n",
    "\n",
    "# ▼ Step 2: Initialize tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def encode(text): return enc.encode(text)\n",
    "def decode(ids): return enc.decode(ids)\n",
    "\n",
    "print(\"Tokenizer vocab size:\", enc.n_vocab)\n",
    "\n",
    "# ▼ Step 3: Load model training data and encode\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_text = f.read()\n",
    "ids = encode(train_text)\n",
    "data = torch.tensor(ids, dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# ▼ Step 4: Batching\n",
    "def get_batch(split):\n",
    "    data_ = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data_[i+1:i+1+block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# ▼ Step 5: Define Transformer model (similar to your previous code)\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x); q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(enc.n_vocab, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, enc.n_vocab)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# ▼ Step 6: Train\n",
    "model = BigramLanguageModel().to(device)\n",
    "print(\"Parameters:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(max_iters):\n",
    "    if it % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {it}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# ▼ Step 7: Generation\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "output = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "print(\"Generated:\\n\", decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f8af6-7884-46be-a078-840fc58ef2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93321de4-bf51-4979-a280-a08fdd107844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
